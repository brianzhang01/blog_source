---
title: Clustering with K-Means and EM
author: Brian Zhang
date: '2018-01-23'
slug: clustering-with-k-means-and-em
categories: []
tags: []
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Hi! This is a post.</p>
<p>Our data comes from Barbara Englehardt’s Spring 2013 Duke course, <a href="https://www.cs.princeton.edu/~bee/courses/cbb540.html">STA613/CBB540: Statistical methods in computational biology</a>, homework 4.</p>
</div>
<div id="load-the-data" class="section level2">
<h2>Load the data</h2>
<p>First, we load our library functions and data points:</p>
<pre class="r"><code>library(deldir)
library(ellipse)
library(pryr)
library(ggplot2)
center_title = theme(plot.title = element_text(hjust = 0.5))
no_legend = theme(legend.position=&quot;none&quot;)</code></pre>
<pre class="r"><code>points = read.table(&#39;../data/points_hw4.txt&#39;, col.names=c(&quot;x&quot;, &quot;y&quot;))
ggplot(points, aes(x = x, y = y)) + geom_point() +
  labs(title = &quot;Scatter plot of data&quot;) +
  center_title</code></pre>
<p><img src="/post/2018-01-23-clustering-with-k-means-and-em_files/figure-html/points-1.png" width="672" /></p>
</div>
<div id="clustering-algorithms" class="section level2">
<h2>Clustering algorithms</h2>
<p>Talk about how we structure the code</p>
<pre class="r"><code>iterate_em = function(nsteps, K, points, e_step, m_step, m_params.init) {
  m_list = list(m_params.init)
  e_list = list()
  i = 1
  while (i &lt;= nsteps) {
    e_list[[i]] = e_step(K, points, m_list[[i]])
    m_list[[i+1]] = m_step(K, points, e_list[[i]])
    i = i + 1
  }
  return(list(m_list=m_list, e_list=e_list))
}</code></pre>
<p>Algorithms:</p>
<pre class="r"><code># K-means as functions
# points: N x D matrix
# e_params: list with
#   clusters: vector of assignments
# m_params: list with
#   centers: matrix of cluster centers
kmeans.e = function(K, points, m_params) {
  N = dim(points)[1]
  D = dim(points)[2]
  
  distances2 = matrix(0, N, K)
  for (k in 1:K) {
    for (j in 1:D) {
      distances2[,k] = distances2[,k] + (points[,j] - m_params$centers[k,j])^2
    }
  }
  clusters = apply(distances2, 1, which.min)
  e_params = list(clusters=clusters)
  
  return(e_params)
}

kmeans.m = function(K, points, e_params) {
  N = dim(points)[1]
  D = dim(points)[2]
  
  centers = matrix(0, K, D)
  for (k in 1:K) {
    centers[k,] = colMeans(points[e_params$clusters == k,])
  }
  m_params = list(centers=centers)
  
  return(m_params)
}

# EM as functions
# points: N x D matrix
# m_params: list with
#   mu: K x D, MoG centers
#   sigma: list of length K of D x D matrices, MoG covariances
#   weights: K, MoG weights
# e_params: list with
#   resp: responsibilities, N x K
#   ll: log-likelihood, for debugging
em.e = function(K, points, m_params) {
  N = dim(points)[1]
  D = dim(points)[2]
  mu = m_params$mu
  sigma = m_params$sigma
  weights = m_params$weights
  
  # update responsibilities
  resp = matrix(rep(0, N*K), N, K)
  for (k in 1:K) {
    constant_k = log(weights[k]) - 0.5*log(det(sigma[[k]])) - log(2*pi)*(D/2)
    displacement = points - as.numeric(matrix(mu[k,], N, D, byrow = TRUE))
    log_probs = -1/2 * colSums(t(displacement) * (solve(sigma[[k]]) %*% t(displacement)))
    resp[,k] = exp(log_probs + constant_k)
  }
  ll = mean(log(rowSums(resp)))  # log likelihood, a useful diagnostic
  resp = resp / matrix(rowSums(resp), N, K)

  e_params = list(resp=resp, ll=ll)
  return(e_params)
}

em.m = function(K, points, e_params, fix_sigma=NULL, fix_weights=NULL) {
  N = dim(points)[1]
  D = dim(points)[2]
  resp = e_params$resp
  
  # update means
  mu = matrix(0, K, D)
  for (k in 1:K) {
    mu[k,] = colSums(resp[,k]*points) / sum(resp[,k])
  }

  # update covarainces
  if (is.null(fix_sigma)) {
    sigma = NULL
    for (k in 1:K) {
      sigma[[k]] = matrix(0, D, D)
      displacement = points - as.numeric(matrix(mu[k,], N, D, byrow = TRUE))
      for (j in 1:D) {
        sigma[[k]][j,] = colSums(displacement[,j]*displacement*resp[,k]) / sum(resp[,k])
      }
    }
  } else {
    sigma = fix_sigma
  }
  
  # update component weights
  if (is.null(fix_weights)) {
    weights = colSums(resp) / sum(resp)
  } else {
    weights = fix_weights
  }
  
  m_params = list(mu=mu, sigma=sigma, weights=weights)
  return(m_params)
}</code></pre>
</div>
<div id="initial-run" class="section level2">
<h2>Initial run</h2>
<pre class="r"><code># Run K means and EM
K = 3
nsteps = 20
N = dim(points)[1]
D = dim(points)[2]
set.seed(3)
centers = points[sample(1:N, K),]
row.names(centers) = NULL
m_params.init = list(centers=centers)
kmeans_results = iterate_em(nsteps, K, points, kmeans.e, kmeans.m, m_params.init)

mu = centers
sigma = NULL
for(k in 1:K) {
  sigma[[k]] = diag(D)  # convariances initialized to identity matrix
}
weights = rep(1, K) / K  # weights initialized to uniform
m_params.init = list(mu=mu, sigma=sigma, weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, em.m, m_params.init)</code></pre>
<p>The results of each E-step produces data for each of the N points, which is verbose. Instead, let’s print the results of the M-step, which is more compact.</p>
<pre class="r"><code>kmeans_results$m_list[1:3]</code></pre>
<pre><code>## [[1]]
## [[1]]$centers
##          x        y
## 1 4.236116 4.322595
## 2 3.115771 3.307241
## 3 4.474370 2.387970
## 
## 
## [[2]]
## [[2]]$centers
##          [,1]     [,2]
## [1,] 3.903263 4.585723
## [2,] 2.434461 3.479530
## [3,] 4.907858 2.282699
## 
## 
## [[3]]
## [[3]]$centers
##          [,1]     [,2]
## [1,] 3.837077 4.520047
## [2,] 2.366460 3.438372
## [3,] 4.907858 2.282699</code></pre>
<pre class="r"><code>em_results$m_list[1:3]</code></pre>
<pre><code>## [[1]]
## [[1]]$mu
##          x        y
## 1 4.236116 4.322595
## 2 3.115771 3.307241
## 3 4.474370 2.387970
## 
## [[1]]$sigma
## [[1]]$sigma[[1]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## [[1]]$sigma[[2]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## [[1]]$sigma[[3]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## 
## [[1]]$weights
## [1] 0.3333333 0.3333333 0.3333333
## 
## 
## [[2]]
## [[2]]$mu
##          [,1]     [,2]
## [1,] 3.596083 4.280319
## [2,] 2.652853 3.548945
## [3,] 4.318030 2.668293
## 
## [[2]]$sigma
## [[2]]$sigma[[1]]
##            [,1]       [,2]
## [1,] 0.66681006 0.05709793
## [2,] 0.05709793 0.56318406
## 
## [[2]]$sigma[[2]]
##           [,1]      [,2]
## [1,] 0.6406161 0.1080031
## [2,] 0.1080031 0.5753433
## 
## [[2]]$sigma[[3]]
##            [,1]       [,2]
## [1,]  1.3820866 -0.4615455
## [2,] -0.4615455  0.7275576
## 
## 
## [[2]]$weights
## [1] 0.3032386 0.5042118 0.1925496
## 
## 
## [[3]]
## [[3]]$mu
##          [,1]     [,2]
## [1,] 3.602189 4.367311
## [2,] 2.578687 3.553098
## [3,] 4.475524 2.552581
## 
## [[3]]$sigma
## [[3]]$sigma[[1]]
##           [,1]      [,2]
## [1,] 0.5694985 0.1191194
## [2,] 0.1191194 0.4292038
## 
## [[3]]$sigma[[2]]
##           [,1]      [,2]
## [1,] 0.5033149 0.1763752
## [2,] 0.1763752 0.5461334
## 
## [[3]]$sigma[[3]]
##            [,1]       [,2]
## [1,]  1.2389029 -0.4628892
## [2,] -0.4628892  0.5710045
## 
## 
## [[3]]$weights
## [1] 0.3006975 0.5026309 0.1966716</code></pre>
<p>Looks sensible. It’s also a good idea to check that the log-likelihood always increases.</p>
<pre class="r"><code>lls = rep(0, nsteps)
for (i in 1:nsteps) {
  lls[i] = em_results$e_list[[i]]$ll
}
ggplot(data=data.frame(x=1:nsteps, y=lls)) +
  geom_line(aes(x=x, y=y)) + geom_point(aes(x=x, y=y)) +
  labs(title=&quot;Log likelihood for EM&quot;, x=&quot;step&quot;, y=&quot;log likelihood&quot;) +
  center_title</code></pre>
<p><img src="/post/2018-01-23-clustering-with-k-means-and-em_files/figure-html/log-likelihood-1.png" width="672" /></p>
</div>
<div id="visualization-code" class="section level2">
<h2>Visualization code</h2>
<pre class="r"><code>make_visualization = function(points, kmeans_data, em_data, nsteps, K) {
  for (i in 1:nsteps) {
    # colored points
    df_points = rbind(
      data.frame(x = points[,1], y = points[,2], type = &quot;K-means&quot;,
                 cluster = kmeans_data$e_list[[i]]$clusters),
      data.frame(x = points[,1], y = points[,2], type = &quot;EM&quot;,
                 cluster = apply(em_data$e_list[[i]]$resp, 1, which.max)))

    # K-means decision boundaries
    centers = kmeans_data$m_list[[i]]$centers
    df_voronoi = deldir(centers[,1], centers[,2])$dirsgs
    df_voronoi$type = factor(&quot;K-means&quot;, levels=c(&quot;K-means&quot;, &quot;EM&quot;))
    
    # ellipses
    mu = em_data$m_list[[i]]$mu
    sigma = em_data$m_list[[i]]$sigma
    all_ellipses = NULL
    for (k in 1:K) {
      ellipse_data = ellipse(sigma[[k]], level=pchisq(1, df=D))
      all_ellipses[[k]] = data.frame(
        x=ellipse_data[,1] + mu[k,1], y=ellipse_data[,2] + mu[k,2], cluster=k, type=&quot;EM&quot;)
    }
    df_ellipses = do.call(rbind, all_ellipses)
    
    print(ggplot() +
            geom_point(data=df_points, aes(x=x, y=y, color=factor(cluster))) +
            geom_point(data=data.frame(x=centers[,1], y=centers[,2], type=&quot;K-means&quot;),
                       aes(x=x, y=y), shape=17, size=3) +
            geom_segment(data=df_voronoi, linetype = 1, color= &quot;#FFB958&quot;,
                         aes(x = x1, y = y1, xend = x2, yend = y2)) +
            geom_path(data=df_ellipses, aes(x=x, y=y, color=factor(cluster))) +
            facet_grid(. ~ type) +
            ggtitle(paste0(&quot;Most likely cluster, K = &quot;, K, &quot;, step = &quot;, i)) +
            center_title + no_legend)
  }
}</code></pre>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<pre class="r"><code># make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<pre class="r"><code># fixed_sigma1 = partial(em.m, fix_sigma = diag(D), fix_weights=weights)
# em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma1, m_params.init)
# make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<pre class="r"><code># sigma = NULL
# for(k in 1:K) {
#   sigma[[k]] = as.numeric(diag(D)*0.1)
# }
# m_params.init = list(mu=mu, sigma=sigma, weights=weights)
# fixed_sigma01 = partial(em.m, fix_sigma = diag(D)*0.1, fix_weights=weights)
# em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma01, m_params.init)
# make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
</div>
<div id="extra-figures" class="section level2">
<h2>Extra figures</h2>
<p>We can repeat this entire process for a different value of <span class="math inline">\(K\)</span>.</p>
<pre class="r"><code># make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<pre class="r"><code># fixed_sigma1 = partial(em.m, fix_sigma = sigma[[1]], fix_weights=weights)
# em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma1, m_params.init)
# make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<pre class="r"><code># sigma = NULL
# for(k in 1:K) {
#   sigma[[k]] = as.numeric(diag(D)*0.1)
# }
# m_params.init = list(mu=mu, sigma=sigma, weights=weights)
# fixed_sigma01 = partial(em.m, fix_sigma = sigma[[1]], fix_weights=weights)
# em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma01, m_params.init)
# make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
</div>
